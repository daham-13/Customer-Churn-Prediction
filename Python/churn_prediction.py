# -*- coding: utf-8 -*-
"""Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13c_L7iC9eewhX40-EqDAUjfJto4JSetl
"""

import pandas as pd
import matplotlib.pyplot as plt

url = "https://raw.githubusercontent.com/daham-13/Customer-Churn-Prediction/refs/heads/main/Data/WA_Fn-UseC_-Telco-Customer-Churn.csv"
data = pd.read_csv(url, sep=",")
print(data.head())
data.info()

print(data['Churn'].value_counts(normalize=True))
print(data.isnull().sum())

import seaborn as sns
data.drop(columns=['customerID'], inplace=True)
data['Churn'] = data['Churn'].map({'No': 0, 'Yes': 1})
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data.dropna(inplace=True)
data.info()
numerical = data.select_dtypes(include=['int64', 'float64'])
correlation_matix = data[numerical.columns].corr()
sns.heatmap(correlation_matix, annot=True, cmap='coolwarm')
plt.show()

numerical.hist(bins=15, figsize=(9, 9))
plt.suptitle("Numerical Features Distributions")
plt.show()

categorical_columns = data.select_dtypes(include=["object"]).columns
for col in categorical_columns:
    data[col].value_counts().plot(kind='bar', figsize=(7.5, 5), title=f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.show()

#Handling skew data
for feature in numerical:
  skewness = data[feature].skew()
  print(f"Skewness of {feature}: {skewness:.2f}")

#Elminating the skewness of Total Charges

import numpy as np

data['totalcharges_transformed'] = np.sqrt(data['TotalCharges'])
print("Skewness after sqrt:", data['totalcharges_transformed'].skew())

data['totalcharges_transformed'].hist(bins=15, figsize=(9, 9))
plt.suptitle("Numerical Features Distributions")
plt.show()

#checking outliers
cols = ['tenure', 'MonthlyCharges', 'totalcharges_transformed']

for item in cols:
  sns.boxplot(data=data[item])
  plt.title(item)
  plt.show()

lable_encode_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']
from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to binary columns
for col in lable_encode_cols:
    data[col] = label_encoder.fit_transform(data[col])

new_cat_coloumns = data.select_dtypes(include=["object"]).columns

data_encoded = pd.get_dummies(data, columns = new_cat_coloumns, drop_first=True)

data_encoded.info()

from sklearn.preprocessing import StandardScaler

# Select numerical columns
numerical_cols = ['tenure', 'MonthlyCharges', 'totalcharges_transformed']

# Initialize scaler
scaler = StandardScaler()

# Scale numerical columns
data_encoded[numerical_cols] = scaler.fit_transform(data_encoded[numerical_cols])

data_encoded.info()

#split data

# Separate features and target
X = data_encoded.drop(['Churn', 'TotalCharges'], axis=1)
y = data_encoded['Churn']

# Verify the final dataset
print(X.info())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.info()
y_train.info()
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(y_train_resampled.info(), y_train.info())

#Train model

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Train logistic regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

# Evaluate on test data
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Report:")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Train logistic regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train_resampled, y_train_resampled)

# Evaluate on test data
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Report:")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))

from xgboost import XGBClassifier

# Train XGBoost
xgb = XGBClassifier(random_state=42)
xgb.fit(X_train_resampled, y_train_resampled)

# Evaluate on test data
y_pred_xgb = xgb.predict(X_test)
print("XGBoost Report:")
print(classification_report(y_test, y_pred_xgb))

from sklearn.metrics import accuracy_score

# Training accuracy
y_train_pred_lr = lr.predict(X_train_resampled)
train_accuracy_lr = accuracy_score(y_train_resampled, y_train_pred_lr)
print("Logistic Regression Training Accuracy:", train_accuracy_lr)

# Testing accuracy
y_test_pred_lr = lr.predict(X_test)
test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)
print("Logistic Regression Testing Accuracy:", test_accuracy_lr)

# Training accuracy
y_train_pred_xgb = xgb.predict(X_train_resampled)
train_accuracy_xgb = accuracy_score(y_train_resampled, y_train_pred_xgb)
print("XGBoost Training Accuracy:", train_accuracy_xgb)

# Testing accuracy
y_test_pred_xgb = xgb.predict(X_test)
test_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)
print("XGBoost Testing Accuracy:", test_accuracy_xgb)

#Hyperparameter tuning for XGBoost

parameter_grid = {
    #tree stucture
    'max_depth' : [3,5],
    'min_child_weight': [1, 5],

    #regulaization
    'reg_alpha': [0, 0.1],              # L1 regularization
    'reg_lambda': [0.5, 1],             # L2 regularization

    # Learning rate and boosting rounds
    'learning_rate': [0.01, 0.1],       # Lower learning rate for better generalization
    'n_estimators': [100, 200],         # Number of trees

    # Random subsampling
    'subsample': [0.8, 1],              #% of training data sampled per tree
    'colsample_bytree': [0.8, 1]        #% of features sampled per tree
}

'''from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=parameter_grid,
    scoring='recall',  # Focus on minimizing false negatives
    cv=5,              # 5-fold cross-validation
    n_jobs=-1
)
grid_search.fit(X_train_resampled, y_train_resampled)

print("Best parameters:", grid_search.best_params_)
print("Best recall score:", grid_search.best_score_)
'''
# Get the best model

params = {
    'colsample_bytree': 1,           # Fraction of features to use for each tree
    'learning_rate': 0.1,            # Controls the contribution of each tree to the final prediction
    'max_depth': 5,                  # Maximum depth of each tree
    'min_child_weight': 1,           # Minimum sum of instance weight for child nodes
    'n_estimators': 200,             # Number of boosting rounds (trees)
    'reg_alpha': 0,                  # L1 regularization term
    'reg_lambda': 1,                 # L2 regularization term
    'subsample': 1                   # Fraction of training data to use for each tree
}

best_xgb = XGBClassifier(
    colsample_bytree=params['colsample_bytree'],
    learning_rate=params['learning_rate'],
    max_depth=params['max_depth'],
    min_child_weight=params['min_child_weight'],
    n_estimators=params['n_estimators'],
    reg_alpha=params['reg_alpha'],
    reg_lambda=params['reg_lambda'],
    subsample=params['subsample'],
    random_state=42
)

# Train with best parameters
best_xgb.fit(X_train_resampled, y_train_resampled)

# Training accuracy
y_train_pred = best_xgb.predict(X_train_resampled)
train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
print("Training Accuracy:", train_accuracy)

# Testing accuracy
y_test_pred = best_xgb.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Testing Accuracy:", test_accuracy)

# Classification report (focus on recall for churn class)
print(classification_report(y_test, y_test_pred))

# ROC-AUC
y_test_proba = best_xgb.predict_proba(X_test)[:, 1]
print("ROC-AUC:", roc_auc_score(y_test, y_test_proba))

import shap

explainer = shap.TreeExplainer(best_xgb)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# Force plot for a single prediction
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0], feature_names=X.columns)

import joblib
import shap

# Save the trained model
joblib.dump(best_xgb, 'churn_model.pkl')

# Save the SHAP explainer
explainer = shap.TreeExplainer(best_xgb)
joblib.dump(explainer, 'shap_explainer.pkl')